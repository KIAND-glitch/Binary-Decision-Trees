{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Binary Decision Tree (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this assignment, you will build the binary Decision Tree (DT) classification algorithm from scratch and apply it to a real-world machine learning dataset. You are not allowed to use third party library functions and directly call pre-built DT algorithms. After building the DT, you will predict the predominant color of national flags using a diverse set of features, including country features such as language, population, in addition to other structural properties. The list of classes (colors) is:\n",
    "\n",
    "```\n",
    "black\n",
    "blue\n",
    "brown\n",
    "gold\n",
    "green\n",
    "orange\n",
    "red\n",
    "white\n",
    "```\n",
    "\n",
    "We use a modified version of the Flags data set from the UCI Machine learning repository <a href=\"https://archive.ics.uci.edu/ml/datasets/Flags\"> Flags Dataset</a> (click the link for more information on all features and values).\n",
    "\n",
    "\n",
    "In the **Data Preparation** section, you will read the dataset into a data frame(Q1.1). You will also need to develop code to convert two of the numeric features to nominal (Q1.2). Then, the data will be divided into two parts: Train and Test. (Q1.3)\n",
    "\n",
    "In the **Model Training** section, you will write a number of helper functions that will be incorporated into the main Binary Decision Tree function (Q2-6).\n",
    "\n",
    "You can observe the use of each of these functions in the Main Binary Decision Tree Algorithm, which you can use to print your trained tree. We have also provided the code for navigating a trained tree and using it to predict the labels for the test dataset.\n",
    "\n",
    "In the **Classification and Evaluation** section, you will test and evaluate your model. You will implement functions from scratch to calculate the accuracy of the model predictions (Q7). *For the evaluation part you are NOT allowed to use third party provided library functions such as classification_report.* Finally, you will train a tree and use it to predict the labels for the test set and see the accuracy of your model (Q8).\n",
    "\n",
    "Q9 includes some analytical questions in which you will critically analyze the behaviour of your tree in different situations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation \n",
    "\n",
    "The dataset that you have consist of a dataset with 194 flags. Each flag is defined by 26 features and 1 `label`. These features are `index`, `landmass`,\t`zone`,\t`area`,\t..., `animate`,\tand `text`.\n",
    "The first feature is only an index and is not useful for classification, but the rest of the features can be used for predicting the color of the flag, i.e., `label`. \n",
    "\n",
    "\n",
    "#### Q1.1 Reading the input file and data preprocessing (0.25 marks)\n",
    "\n",
    "This code should read the input file into a `pandas` dataframe. You should remove the first feature because it is only an id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ YOUR CODE HERE ##############\n",
    "\n",
    "# First, read the input file\n",
    "data_p = pd.read_csv('flags.data.csv', sep=',')\n",
    "# print(data_p.head())\n",
    "\n",
    "# Second, drop the index column\n",
    "df = data_p.iloc[:, 1:]\n",
    "    \n",
    "# deep copy creation for equal width binning\n",
    "deep_copy_df = df.copy(deep=True)\n",
    "\n",
    "############## TEST IT YOURSELF ###############\n",
    "\n",
    "assert df['landmass'][0] == 5\n",
    "assert df['zone'][67] == 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.2 Convert Numeric features to Categorical (0.5 marks)\n",
    "In order to simplify our calculations we are going to transform the `area` and `population` features with numeric values to categories. You will be using **equal frequency binning with 5 bins**.\n",
    "\n",
    "**NOTE:** Assign sequential integer values starting from 0 to 4 to each of the bins. For example for `population` feature, any value between [0,1] is mapped to 0, any value between (1,4] is mapped to 1, and so on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ YOUR CODE HERE ##############\n",
    "\n",
    "\n",
    "df['area'] = pd.qcut(df['area'].rank(method='first'), q=5, labels=False)\n",
    "\n",
    "df['population'] = pd.qcut(df['population'].rank(method='first'), q=5, labels=False)\n",
    "\n",
    "# print(np.array(df['area']))\n",
    "# print(np.array(df['population']))\n",
    "\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.3 Split the data into a Train and Test Set (0.25 marks)\n",
    "The first 150 instances should be used for training and the last 44 instances for testing. **SHUFFLING IS NOT ALLOWED!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first 150 instances should be used for training and the last 44 instances for testing\n",
    "# SHUFFLING IS NOT ALLOWED\n",
    "\n",
    "train_df = df.iloc[:150]\n",
    "test_df = df.iloc[150:]\n",
    "\n",
    "############## TEST IT YOURSELF ###############\n",
    "assert(len(train_df)==150)\n",
    "assert(len(test_df)==44)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Binary Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "There are a number of helper functions that we provide here. In addition to the following helper functions, you must implement a few other helper functions to use in your binary decision tree algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "# This function receives instances in a 2D array format \n",
    "# and returns the most common label for the passed instances. \n",
    "\n",
    "def assign_label(data):\n",
    "    # the input `data` is a 2D array\n",
    "    # the output is a STRING \n",
    "    \n",
    "    column = data[:, -1] \n",
    "    \n",
    "    values, counts = np.unique(column, return_counts=True)\n",
    "    index = counts.argmax()\n",
    "    name = values[index]\n",
    "    \n",
    "    return name\n",
    "\n",
    "#######################################################################################\n",
    "# This function separates instances (data) based on a given feature (split feature)\n",
    "# and a threshold value (value). \n",
    "# it returns a left and right subtree corresponding to that threshold value.\n",
    "\n",
    "def find_subtrees(data, features_list, split_feature, value):  \n",
    "    # the input `data` is a 2D array \n",
    "    # the input `features_list` is the list of feature names\n",
    "    # the input `split_feature` specifies a single feature name\n",
    "    # the input `value` specifies a value for the `split_feature`\n",
    "    # the outputs `data_left` and `data_right` are 2D arrays (subtrees) \n",
    "    \n",
    "    index = features_list.get_loc(split_feature)\n",
    "    data_left = data[data[:,index]<value]\n",
    "    data_right = data[data[:,index]>=value]\n",
    "    \n",
    "    return data_left, data_right\n",
    "\n",
    "#######################################################################################\n",
    "# This function computes the entropy of a set of instances (data) \n",
    "\n",
    "def calc_entopy(data):\n",
    "    # the input `data` is a 2D array \n",
    "    # the output is a single real-valued number\n",
    "    \n",
    "    column = data[:, -1]\n",
    "    _, counts = np.unique(column, return_counts=True)\n",
    "\n",
    "    x = counts / counts.sum()  \n",
    "    y = sum(x * -np.log2(x))\n",
    "    \n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2. Write a function that will determine whether a node is pure or not. (0.5 marks)**\n",
    "\n",
    "It takes as input: \n",
    "- data: 2D array of values containing instances\n",
    "\n",
    "It returns as output: \n",
    "- answer: a boolean value indicating whether the data (node) is pure or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function receives a 2D array of values containing instances, it should have a format such as follows:\n",
    "#  [[3, 1, 1, ..., 1, 0, 'red'],\n",
    "#   [4, 1, 4, ..., 0, 0, 'green'],\n",
    "#   [1, 1, 0, ..., 0, 0, 'red'],\n",
    "#   ...,\n",
    "#   [5, 1, 3, ..., 0, 0, 'red']]\n",
    "\n",
    "\n",
    "def calc_purity(data): \n",
    "    # the input `data` is a 2D array as illustrated above\n",
    "    # the output `answer` is a BOOLEAN (True or False)\n",
    "    \n",
    "    ############ YOUR CODE HERE ##############\n",
    "\n",
    "    column = data[:, -1] \n",
    "    \n",
    "    answer = np.unique(column).size == 1\n",
    "        \n",
    "    ##########################################\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## TEST IT YOURSELF ###############\n",
    "assert calc_purity(df.values) == False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3. Write a function that will determine the mean information for a given set of instances. (0.75 marks)**\n",
    "\n",
    "It takes as input: \n",
    "- data: an array of 2D arrays of values containing instances\n",
    "\n",
    "It returns as output: \n",
    "- mi: mean information which is a single real-valued number, refer to lecture 4 slides for Mean info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function receives an array of 2D arrays of values containing instances, it should have a format such as follows:\n",
    "# [[[3, 1, 1, ..., 1, 0, 'red'],\n",
    "#   [4, 1, 4, ..., 0, 0, 'green'],\n",
    "#   [1, 1, 0, ..., 0, 0, 'red'],\n",
    "#   ...,\n",
    "#   [5, 1, 3, ..., 0, 0, 'red'],\n",
    "\n",
    "#   [[6, 3, 0, ..., 1, 0, 'blue'],\n",
    "#    [2, 3, 4, ..., 0, 0, 'gold'],\n",
    "#    [2, 3, 4, ..., 0, 0, 'blue'],\n",
    "#    ...,\n",
    "#    [3, 3, 3, ..., 0, 0, 'blue']]]\n",
    "\n",
    "\n",
    "def calculate_MI(data):\n",
    "    # the input `data` is an array of 2D arrays as illustrated above\n",
    "    # the output `mi` is a real-valued number\n",
    "    ############ YOUR CODE HERE ##############\n",
    "    \n",
    "    n = 0\n",
    "    for arr in data:\n",
    "        n += len(arr)\n",
    "\n",
    "    mi = 0\n",
    "    for arr in data:\n",
    "        mi += (len(arr) / n) * calc_entopy(arr)\n",
    "    \n",
    "    ##########################################\n",
    "    \n",
    "    return mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## TEST IT YOURSELF ###############\n",
    "assert(calculate_MI([df[df['label']=='red'].values,df[df['label']=='red'].values]))==0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4. Write a function that will determine the information gain after we perform a split for a feature (0.5 marks)**\n",
    "\n",
    "It takes as input: \n",
    "-  root: the data before the split\n",
    "-  children: the subtrees after the split\n",
    "\n",
    "\n",
    "It returns as output: \n",
    "- ig: Information Gain which is a single real-valued number, please refer to lecture 4 slides for information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_IG(root, children):\n",
    "    # the input `root` is a 2D array as illustrated in Q2\n",
    "    # the input `children` is an array of 2D arrays as illustared in Q3\n",
    "    # the output `ig` is a real-valued number\n",
    "    ############ YOUR CODE HERE ##############\n",
    "\n",
    "    ig = calc_entopy(root) - calculate_MI(children)\n",
    "    \n",
    "    ##########################################\n",
    "    \n",
    "    return ig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5. Write a function that will determine the best feature and value combination that results in the highest information gain (0.75 marks)**\n",
    "\n",
    "It takes as input: \n",
    "-  data: the data before the split\n",
    "-  features_list: the list of feature names\n",
    "\n",
    "\n",
    "It returns as output: \n",
    "- y1: name of the feature with the best information gain ( String ) \n",
    "- y2: value of the feature with the best information gain ( a feature value )\n",
    "\n",
    "**Note: use the find_subtrees(data, feature_list, split_feature, value) function for all possible combinations of <feature,value> to determine the best <feature,value> combination resulting in the highest information gain** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_IG(data,features_list):\n",
    "    # the input `data` is a 2D array as illustrated in Q2\n",
    "    # the input `feature_list` is a list of feature names in the format of pandas Index\n",
    "    # the output `y1` is a String\n",
    "    # the output y2 is a numeric feature value\n",
    "    ############ YOUR CODE HERE ##############\n",
    "    \n",
    "    max_ig = 0\n",
    "    \n",
    "    for feature in features_list[:-1]:\n",
    "\n",
    "        values = np.unique(data[:, features_list.get_loc(feature)])\n",
    "        for v in values:\n",
    "            \n",
    "            data_left, data_right =  find_subtrees(data, features_list, feature, v)\n",
    "            ig = calculate_IG(data, [data_left, data_right])  \n",
    "            \n",
    "            if ig >= max_ig:\n",
    "                max_ig = ig\n",
    "                y1 = feature\n",
    "                y2 = v\n",
    "    \n",
    "    ##########################################\n",
    "    return y1, y2\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## TEST IT YOURSELF ###############\n",
    "assert(find_best_IG(train_df.values, train_df.columns)==('blue',1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Binary Decision Tree Algorithm\n",
    "**Q6. This is the recursive part of developing the Binary Decision Tree. If you have developed all the previous parts correctly this function will make a Binary Decision Tree for the passed dataset. We can set the maximum depth of the tree (`max_depth`) and number of samples that we would stop splitting a node (`min_samples`). You must edit the code below in the marked area `criteria` for the stopping condition in the ID3 algorithm. (0.5 marks)**\n",
    "\n",
    "The criteria are:\n",
    " - the current node is pure or;\n",
    " - number of instances in the current node is less than minimum samples or;\n",
    " - we have reached the maximum depth of the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_algorithm(data, array, max_depth, counter=0, min_samples=2):\n",
    "    \n",
    "    \n",
    "    criteria =  (calc_purity(data)) or (len(data) < min_samples) or (counter == max_depth)\n",
    "    if criteria :\n",
    "        y = assign_label(data)\n",
    "        return y\n",
    "    \n",
    "    # recursive part\n",
    "    else:\n",
    "        counter += 1\n",
    "        \n",
    "        name, value = find_best_IG(data,array)\n",
    "        data1, data2 = find_subtrees(data, array, name, value)\n",
    "        \n",
    "        if (len(data1) == 0) or (len(data2) == 0):\n",
    "            y = assign_label(data)\n",
    "            return y\n",
    "    \n",
    "        #instansiate the tree\n",
    "        node = \"{} < {}\".format(name, value)\n",
    "        sub_tree = {node: []}\n",
    "              \n",
    "        #develop the sub-trees (recursion)\n",
    "        left_child = decision_tree_algorithm(data=data1, array=array, max_depth=max_depth, counter=counter)\n",
    "        right_child = decision_tree_algorithm(data=data2, array=array, max_depth=max_depth, counter=counter)\n",
    "        \n",
    "        \n",
    "        if left_child == right_child:\n",
    "            sub_tree = right_child\n",
    "        else:\n",
    "            sub_tree[node].append(left_child)\n",
    "            sub_tree[node].append(right_child)\n",
    "        \n",
    "        return sub_tree   \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## TEST IT YOURSELF ###############\n",
    "data = train_df\n",
    "tree = decision_tree_algorithm(data=data.values, array=data.columns, max_depth=2, counter=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Evaluation\n",
    "\n",
    "Now that we have built our binary decision tree, we are ready to use it to predict a label for unseen instances. We provided the code to do this, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one_instance(example, tree):\n",
    "    \n",
    "    # tree is just a root node\n",
    "    if not isinstance(tree, dict):\n",
    "        return tree\n",
    "    \n",
    "    question = list(tree.keys())[0]\n",
    "    feature_name, comparison_operator, value = question.split(\" \")\n",
    "\n",
    "    # ask question\n",
    "    if (example[feature_name] < float(value)):\n",
    "        answer = tree[question][0]\n",
    "    else:\n",
    "        answer = tree[question][1]\n",
    "\n",
    "    # base case (the answer in not a dictionary)\n",
    "    if not isinstance(answer, dict):\n",
    "        return answer\n",
    "    \n",
    "    # recursive part\n",
    "    else:\n",
    "        residual_tree = answer\n",
    "        return predict_one_instance(example, residual_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(df, tree):\n",
    "    \n",
    "    if len(df) != 0:\n",
    "        predictions = df.apply(predict_one_instance, args=(tree,), axis=1)\n",
    "    else:\n",
    "        # \"df.apply()\"\" with empty dataframe returns an empty dataframe,\n",
    "        # but \"predictions\" should be a series instead\n",
    "        predictions = pd.Series()\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7. Write a function to calculate accuracy (0.25 marks)**\n",
    "\n",
    "This function receives a dataframe and predictions of a trained tree (on that dataset) and should calculate the accuracy of the predictions for the passed dataframe.\n",
    "\n",
    "**NOTE:** You are NOT allowed to use any predefined Python accuracy methods here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(df, predictions):\n",
    "    \n",
    "    ########### YOUR CODE HERE ##############\n",
    "    correct_predictions = 0\n",
    "    total_instances = len(df)\n",
    "    label = df['label']\n",
    "        \n",
    "    for i in df.index:\n",
    "        if label[i] == predictions[i]:\n",
    "            correct_predictions += 1\n",
    "    \n",
    "    accuracy = correct_predictions / total_instances\n",
    "    \n",
    "    #########################################\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8. Use the Tree (0.25 marks)**\n",
    "\n",
    "Using all the developed functions above build a tree using the **Training set** (train_df) (max_depth = 3) and then use it to predict labels for the **Test set** (test_df). Then print the accuracy of your tree for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4318181818181818\n"
     ]
    }
   ],
   "source": [
    "#############  YOUR CODE HERE ##############\n",
    "depth_3_tree = decision_tree_algorithm(data=train_df.values, array=train_df.columns, max_depth=3, counter=0)\n",
    "accuracy = calculate_accuracy(test_df, make_predictions(test_df, depth_3_tree))\n",
    "###########################################\n",
    "print(accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9. Analytical questions (5.5 marks)\n",
    "\n",
    "Answer each of the following subquestions with a text answer of 3-5 sentences, using the results you obtained in the previous sections. You might need to re-run and reuse some of the functions you already implemented in the ### CODE ### cells above.\n",
    "\n",
    "\n",
    "### Q9.1. Analysing equal-frequency versus equal-width binning (1.5 marks)\n",
    "\n",
    "**(A)** Write a code to transform the `area` and `population` features with numeric values to categories using **equal width** binning with 5 bins. **[0.5 mark]**\n",
    "\n",
    "**(B)** Name which approach is more appropriate for `area` and `population` features, and explain the reasoning behind your decision.\n",
    "**[1 mark]**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE 9.1(A) ###(A) Write a code to transform the area and population features with numeric values to categories using equal width binning with 5 bins. [0.5 mark]\n",
    "\n",
    "num_bins = 5\n",
    "\n",
    "area_range = deep_copy_df['area'].max() - deep_copy_df['area'].min()\n",
    "area_bin_width = area_range / num_bins\n",
    "area_bin_edges = [deep_copy_df['area'].min() + i * area_bin_width for i in range(num_bins + 1)]\n",
    "\n",
    "deep_copy_df['area'] = pd.cut(deep_copy_df['area'], bins=area_bin_edges, include_lowest=True, labels=False)\n",
    "\n",
    "population_range = deep_copy_df['population'].max() - deep_copy_df['population'].min()\n",
    "population_bin_width = population_range / num_bins\n",
    "population_bin_edges = [deep_copy_df['population'].min() + i * population_bin_width for i in range(num_bins + 1)]\n",
    "\n",
    "deep_copy_df['population'] = pd.cut(deep_copy_df['population'], bins=area_bin_edges, include_lowest=True, labels=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal frequency binning for area\n",
      "Bin 0: Number of samples = 39\n",
      "Bin 1: Number of samples = 39\n",
      "Bin 2: Number of samples = 38\n",
      "Bin 3: Number of samples = 39\n",
      "Bin 4: Number of samples = 39\n",
      "\n",
      "Equal frequency binning for population\n",
      "Bin 0: Number of samples = 39\n",
      "Bin 1: Number of samples = 39\n",
      "Bin 2: Number of samples = 38\n",
      "Bin 3: Number of samples = 39\n",
      "Bin 4: Number of samples = 39\n",
      "\n",
      "Equal width binning for area\n",
      "Bin 0: Number of samples = 188\n",
      "Bin 1: Number of samples = 2\n",
      "Bin 2: Number of samples = 3\n",
      "Bin 4: Number of samples = 1\n",
      "\n",
      "Equal width binning for population\n",
      "Bin 0: Number of samples = 194\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Additional code to support text answer below\n",
    "\n",
    "print('Equal frequency binning for area')\n",
    "equal_freq_area_bins = df['area'].value_counts().sort_index()\n",
    "\n",
    "# Print bins and number of samples\n",
    "for bin_label, count in equal_freq_area_bins.items():\n",
    "    print(f\"Bin {bin_label}: Number of samples = {count}\")\n",
    "print()\n",
    "\n",
    "print('Equal frequency binning for population')\n",
    "equal_freq_population_bins = df['population'].value_counts().sort_index()\n",
    "\n",
    "# Print bins and number of samples\n",
    "for bin_label, count in equal_freq_population_bins.items():\n",
    "    print(f\"Bin {bin_label}: Number of samples = {count}\")\n",
    "print()\n",
    "\n",
    "print('Equal width binning for area')\n",
    "equal_width_area_bins = deep_copy_df['area'].value_counts().sort_index()\n",
    "\n",
    "# Print bins and number of samples\n",
    "for bin_label, count in equal_width_area_bins.items():\n",
    "    print(f\"Bin {bin_label}: Number of samples = {count}\")\n",
    "print()\n",
    "\n",
    "print('Equal width binning for population')\n",
    "equal_width_population_bins = deep_copy_df['population'].value_counts().sort_index()\n",
    "\n",
    "# Print bins and number of samples\n",
    "for bin_label, count in equal_width_population_bins.items():\n",
    "    print(f\"Bin {bin_label}: Number of samples = {count}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Type your text answers here**\n",
    "\n",
    "**(B)**\n",
    "\n",
    "\n",
    "Equal frequency binning is more appropriate as although both maintain a balanced distribution of samples for the area feature, this is not case for the population feature (this can be seen from the results displayed above, which shows the binning type, feature, and the number of samples in each bin). Transforming the population feature with numeric values to categories using equal width binning with 5 bins, the number of samples in each bin varies widely, and the is unevenly distributed across bins.\n",
    "\n",
    "This is due to there being outliers in the population feature with the max value being for Item 38, which has the value of 1008, while most of the other values are significantly smaller. This leads to majority of the sample populating Bin 0.\n",
    "\n",
    "Using Equal Frequency binning, the number of samples in each bin is consistent with the same number of samples in most bins, and therefore will be more appropriate for our given dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9.2. Analysing model accuracy (2.25 marks)\n",
    "\n",
    "**(A)** Plot a <a href=\"https://en.wikipedia.org/wiki/Histogram\">histogram</a> of the actual class frequencies in the test set, and a histogram of the predicted test labels for the decision tree with depth=3. You should produce **a single plot** which shows the histogram both true and predicted labels. You should label the x-axis and y-axis appropriately and use legends to make your plot readable. [*N.B. you may use libraries like <a href=\"https://matplotlib.org/stable/tutorials/introductory/usage.html#sphx-glr-tutorials-introductory-usage-py\">matplotlib</a> or <a href=\"https://seaborn.pydata.org/introduction.html\">seaborne</a>*] **[1 mark]**\n",
    "\n",
    "**(B)** Describe and explain the discrepancy between the true and predicted distributions. **[0.75 mark]**\n",
    "\n",
    "**(C)** Do you think the accuracy is an appropriate evaluation metric for the *Flags* data set? Explain your answer. **[0.5 marks]**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "################################################\n",
    "# Your answer to Question 9.2(A) STARTS HERE\n",
    "################################################\n",
    "plt.figure(figsize=(10, 6))\n",
    "true_labels, true_label_counts = np.unique(test_df[\"label\"], return_counts=True)\n",
    "predicted_labels, predicted_label_counts = np.unique(make_predictions(test_df, depth_3_tree), return_counts=True)\n",
    "\n",
    "plt.bar(true_labels, true_label_counts, label=\"True Labels\")\n",
    "plt.bar(predicted_labels, predicted_label_counts, label=\"Predicted Labels\", width=0.4)\n",
    "\n",
    "plt.xlabel('Class Labels')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.title('True and Predicted Class Frequencies for the Test Set')\n",
    "plt.show()\n",
    "  \n",
    "\n",
    "################################################\n",
    "# Your answer to Question 9.2(A) ENDS HERE\n",
    "################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Type your text answers here**\n",
    "\n",
    "**(B)**\n",
    "\n",
    "\n",
    "From the histogram generated above, it is visible that 'white' and 'orange' labels are not being predicted, while the 'brown' label is being predicted even though there is no presence of it in the test set.\n",
    "We can also see that that 'blue' and 'gold' are being predicted more often their true labels, while 'green' and 'red' are being predicted less often.\n",
    "\n",
    "Labels with 'green','red', 'orange', and 'white' are probably being predicted as 'blue', 'gold' and 'brown', which could be the reason for latter being predicted more than the actual number of their true labels.\n",
    "\n",
    "This is the depth 3 tree generated on the train set which was used to make predictions on the test set.\n",
    "\n",
    "{'blue < 1': [{'red < 1': [{'orange < 1': ['green', 'brown']},\n",
    "                           {'gold < 1': ['red', 'gold']}]},\n",
    "              {'red < 1': [{'green < 1': ['blue', 'green']},\n",
    "                           {'white < 1': ['gold', 'blue']}]}]}\n",
    "                           \n",
    "The decision tree reveals that the absence of 'orange' and 'white' labels within the leaf nodes indicates that the tree is not predicting these specific labels.                 \n",
    "The training data set as well as the depth of the tree are quite small, with a very uneven composition of labels, so there is not enough information for predicted the labels accurately.\n",
    "\n",
    "**(C)**\n",
    "\n",
    "The Flags data set involves a multiclass classification problem and accuracy is not the most appropriate evaluation metric since the class distribution is imbalanced with an uneven composition of labels. \n",
    "\n",
    "There is an imbalance in the class distributions, with some label like 'blue' and 'red' being significantly more prevalent in the data set as compared to other classes. Furthermore, it may be the case that different features have different levels of importance when it comes to prediction. \n",
    "\n",
    "Accuracy also does not take into account true positives, false positives, and false negative, which could be more helpful when there is a class imbalance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9.3 Features (0.75 mark)\n",
    "\n",
    "By looking at the `flags.names` file as well as your helper functions answer the following question. Feel free to implement gain ratio but you can answer this question intuitively too. \n",
    "\n",
    "**(A)** If we were to use `gain ratio` instead of `information gain`, which of the 25 attributes would be most affected? why? **0.75**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "landmass has 6 distinct values\n",
      "zone has 4 distinct values\n",
      "area has 5 distinct values\n",
      "population has 5 distinct values\n",
      "language has 10 distinct values\n",
      "bars has 5 distinct values\n",
      "stripes has 12 distinct values\n",
      "colours has 8 distinct values\n",
      "red has 2 distinct values\n",
      "green has 2 distinct values\n",
      "blue has 2 distinct values\n",
      "gold has 2 distinct values\n",
      "white has 2 distinct values\n",
      "black has 2 distinct values\n",
      "orange has 2 distinct values\n",
      "circles has 4 distinct values\n",
      "crosses has 3 distinct values\n",
      "saltires has 2 distinct values\n",
      "quarters has 3 distinct values\n",
      "sunstars has 14 distinct values\n",
      "crescent has 2 distinct values\n",
      "triangle has 2 distinct values\n",
      "icon has 2 distinct values\n",
      "animate has 2 distinct values\n",
      "text has 2 distinct values\n",
      "label has 8 distinct values\n",
      "\n",
      "sunstars has the most number of distinct values: 14\n"
     ]
    }
   ],
   "source": [
    "### CODE 9.3 optional code ###\n",
    "for col in df.columns:\n",
    "    most_distinct_value = df[col].value_counts().idxmax()\n",
    "    num_distinct_values = df[col].nunique()\n",
    "    print(f\"{col} has {num_distinct_values} distinct values\")\n",
    "    \n",
    "print()    \n",
    "most_distinct_column = max(df, key=lambda col: df[col].nunique())\n",
    "print(most_distinct_column,\"has the most number of distinct values:\", df[most_distinct_column].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Type your text answers here**\n",
    "\n",
    "Using Gain Ration instead of Information Gain, reduces the bias for information gain towards highly branching attributes, i.e attributes with more distinct values, by normalising relative to the split information.\n",
    "\n",
    "By looking through the flag.names file, it seems to be the case that the 'languages' feature has a high number of distinct values, with the number of values being 10, but after the analysing the data set, and calculating the distinct values for each, it turns out that the 'sunstars' feature has 14 distinct values, and therefore has the highest number of distinct values.\n",
    "\n",
    "The 'sunstars' feature would be most affected as it has the highest number of distinct values, and gain ratio penalizes attributes with a large number of values, by normalising relative to the split information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9.4 Decision tree complexity (1 mark)\n",
    "\n",
    "**(A)** Using the tree you generated in Q8 (name it `little_tree`), find the accuracy of the tree for predicting the labels for the **training set**. Do you notice a difference between train and test accuracy? If so, discuss possible reasons. \n",
    "\n",
    "**(B)** Now change the max_depth of the decision tree to 10 and train another tree (name it `big_tree`). Now use this new tree to predict the labels for test and train sets. Describe and explain any change in the results you notice compared to your tree of depth 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_little_tree_train:  0.5266666666666666\n",
      "accuracy_little_tree_test:  0.4318181818181818\n"
     ]
    }
   ],
   "source": [
    "### CODE 9.4 (A) ###\n",
    "little_tree = depth_3_tree\n",
    "accuracy_little_tree_train = calculate_accuracy(train_df, make_predictions(train_df, little_tree))\n",
    "accuracy_little_tree_test = calculate_accuracy(test_df, make_predictions(test_df, depth_3_tree))\n",
    "print(\"accuracy_little_tree_train: \", accuracy_little_tree_train)\n",
    "print(\"accuracy_little_tree_test: \", accuracy_little_tree_test)\n",
    "# pprint(little_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_big_tree_train:  1.0\n",
      "accuracy_big_tree_test:  0.45454545454545453\n"
     ]
    }
   ],
   "source": [
    "### CODE 9.4 (B) ###\n",
    "big_tree = decision_tree_algorithm(data=train_df.values, array=train_df.columns, max_depth=10, counter=0)\n",
    "accuracy_big_tree_train = calculate_accuracy(train_df, make_predictions(train_df, big_tree))\n",
    "accuracy_big_tree_test = calculate_accuracy(test_df, make_predictions(test_df, big_tree))\n",
    "print(\"accuracy_big_tree_train: \", accuracy_big_tree_train)\n",
    "print(\"accuracy_big_tree_test: \", accuracy_big_tree_test)\n",
    "# print(big_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Type your text answers here**\n",
    "\n",
    "**(A)**\n",
    "\n",
    "The difference between the train and test accuracies may be due to the tree being trained to a depth of 3, resulting in it being quite shallow and underfitting the data. It is not able to generate appropriate patterns from the train set to make accurate predictions on the test set, which could be resulting in a lower accuracy for the test set as compared to the train set. The training data is limited with only 150 instances, which could lead to the model memorizing instead of generalising based on patterns.\n",
    "\n",
    "There also seems to be a presence of an imbalance of classes, with the distibution of classes in the training and test data being different, and decision tree model may be predicting the majority classes better on the training set, while failing to generalise to the minority in the test set.\n",
    "\n",
    "Another possible reason could due to feature variability, with some features not being well represented in the train set, affecting the predication capability in the test set.\n",
    "\n",
    "**(B)**\n",
    "\n",
    "Changing the max_depth of the tree to 10, the accuracy of the training set has increased significantly, and it predicts labels of the training data accurate with a accuracy of 1 or 100%. There is a slight increase in the accuracy of prediction on the test set to 0.45, but not very significant, and is not able to predict the labels for new instances very well. \n",
    "\n",
    "This could be a result of overfitting, and the model has captured more intricate patterns in the training data, but the improvement towards its test set accuracy is still relatively low, indicating that it is also not generalising well to new data. \n",
    "\n",
    "The tree has also likely learned noise and outliers present in the training data, which is why it achieves this perfect accuracy, but does not perform well on the test data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authorship Declaration:\n",
    "\n",
    "   (1) I certify that the program contained in this submission is completely\n",
    "   my own individual work, except where explicitly noted by comments that\n",
    "   provide details otherwise.  I understand that work that has been developed\n",
    "   by another student, or by me in collaboration with other students,\n",
    "   or by non-students as a result of request, solicitation, or payment,\n",
    "   may not be submitted for assessment in this subject.  I understand that\n",
    "   submitting for assessment work developed by or in collaboration with\n",
    "   other students or non-students constitutes Academic Misconduct, and\n",
    "   may be penalized by mark deductions, or by other penalties determined\n",
    "   via the University of Melbourne Academic Honesty Policy, as described\n",
    "   at https://academicintegrity.unimelb.edu.au.\n",
    "\n",
    "   (2) I also certify that I have not provided a copy of this work in either\n",
    "   softcopy or hardcopy or any other form to any other student, and nor will\n",
    "   I do so until after the marks are released. I understand that providing\n",
    "   my work to other students, regardless of my intention or any undertakings\n",
    "   made to me by that other student, is also Academic Misconduct.\n",
    "\n",
    "   (3) I further understand that providing a copy of the assignment\n",
    "   specification to any form of code authoring or assignment tutoring\n",
    "   service, or drawing the attention of others to such services and code\n",
    "   that may have been made available via such a service, may be regarded\n",
    "   as Student General Misconduct (interfering with the teaching activities\n",
    "   of the University and/or inciting others to commit Academic Misconduct).\n",
    "   I understand that an allegation of Student General Misconduct may arise\n",
    "   regardless of whether or not I personally make use of such solutions\n",
    "   or sought benefit from such actions.\n",
    "\n",
    "   <b>Signed by</b>: [Kian Dsouza - 1142463]\n",
    "   \n",
    "   <b>Dated</b>: [11/08/2023]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
